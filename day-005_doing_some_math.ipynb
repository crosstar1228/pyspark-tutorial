{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5 - Doing Some Math\n",
    "Doing some calculations is a common task for me when I'm writing ETL jobs, e.g. when amounts need to be aligned to the domestic currency (for me mostly EUR) or when I need to unify the scaling of numeric values. The handling of missing values is alsa a regular issuer here. \n",
    "\n",
    "Applying math functions becomes even more impartant to me, when it comes to analytical queries and Key Performance Indicator calculation. So today, I want to have a closer look at the following pyspark sub-modules: \n",
    "* `pyspar.sql.functions`\n",
    "* `pyspark.sql.DataFrameNaFunctions`\n",
    "* `pyspark.sql.DataFrameStatFunctions`\n",
    "\n",
    "## Some Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(\"./data/day-005/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the derived schema and a small data sample shows, that Spark interpretes the Customer ID as as decimal numbers (double). Actually they are integers and so I therefore I want to get rid of the decimals. \n",
    "\n",
    "Rounding to zero decimals is not a good option, because the result is still a double having a decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|round(CustomerID, 0)|       Country|\n",
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "|   580538|    23084|      48|     1.79|             14075.0|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|             14075.0|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|             14075.0|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|             14075.0|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|             14075.0|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|             14075.0|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|             14075.0|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|             14075.0|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|             18180.0|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|             18180.0|United Kingdom|\n",
      "+---------+---------+--------+---------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\", \n",
    "          round(\"CustomerID\",0), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is more an data type issue, rather than a calculation problem, type casting is more appropriate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\", \n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to calculate the amount for each invoice position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|            Amount|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79|             85.92|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|              25.0|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|39.599999999999994|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|              30.0|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|15.299999999999999|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|              40.8|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|              39.6|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69|             40.56|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|              17.0|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|              17.0|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+------------------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\",\n",
    "          (col(\"Quantity\") * col(\"UnitPrice\")).alias(\"Amount\"),\n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I forgot to round the amount to two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+---------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round \n",
    "\n",
    "df.select(\"InvoiceNO\", \n",
    "          \"StockCode\", \n",
    "          \"Quantity\", \n",
    "          \"UnitPrice\",\n",
    "          round((col(\"Quantity\") * col(\"UnitPrice\")), 2).alias(\"Amount\"),\n",
    "          col(\"CustomerID\").cast(\"integer\"), \n",
    "          \"Country\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I save the intermediate results of my data preperation in a variable, to keep the further analytical queries more simple. By decomposing my query into a preperation part and an analytical part, I get the option to check, that the intermediat results are correct and so they a the appropriate foundation of my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf = df.select(\n",
    "    \"InvoiceNO\", \n",
    "    \"InvoiceDate\",\n",
    "    \"StockCode\", \n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    round((col(\"Quantity\") * col(\"UnitPrice\")), 2).alias(\"Amount\"),\n",
    "    col(\"CustomerID\").cast(\"integer\"), \n",
    "    \"Country\")\n",
    "\n",
    "preparedDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "### Aggregating on DataFrames\n",
    "First I want to apply several aggregation functions on the entire dataset in the `DataFrame` to do some data profiling. To make the output more readable, I switch `show()` to vertical output to get many rows instead of many columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " count                 | 541909              \n",
      " countDistinct         | 5827                \n",
      " approx_count_distinct | 5417                \n",
      " sum                   | 9747747.929999992   \n",
      " sumDistinct           | 863586.9200000005   \n",
      " min                   | -168469.6           \n",
      " max                   | 168469.6            \n",
      " avg                   | 17.987794869618316  \n",
      " mean                  | 17.987794869618316  \n",
      " variance              | 143497.64000554013  \n",
      " var_samp              | 143497.64000554013  \n",
      " var_pop               | 143497.37520528768  \n",
      " stddev                | 378.81082350632505  \n",
      " kurtosis              | 151196.60137753483  \n",
      " skewness              | -0.9643865070858197 \n",
      " first                 | 3.26                \n",
      " last                  | 176.48              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"Amount\").alias(\"count\"), \n",
    "        countDistinct(\"Amount\").alias(\"countDistinct\"),\n",
    "        approx_count_distinct(\"Amount\", rsd=0.1).alias(\"approx_count_distinct\"),\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        sumDistinct(\"Amount\").alias(\"sumDistinct\"),\n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\"), \n",
    "        variance(\"Amount\").alias(\"variance\"), \n",
    "        var_samp(\"Amount\").alias(\"var_samp\"),\n",
    "        var_pop(\"Amount\").alias(\"var_pop\"),\n",
    "        stddev(\"Amount\").alias(\"stddev\"),\n",
    "        kurtosis(\"Amount\").alias(\"kurtosis\"),\n",
    "        skewness(\"Amount\").alias(\"skewness\"),\n",
    "        first(\"Amount\").alias(\"first\"),\n",
    "        last(\"Amount\").alias(\"last\")\n",
    "    )\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are further aggregating I don't have a use case in my example here, functions like:\n",
    "* **corr(col1, col2)** - returns a new Column for the Pearson Correlation Coefficient for col1 and col2\n",
    "* **covar_pop(col1, col2)** - returns a new Column for the population covariance of col1 and col2\n",
    "* **covar_samp(col1, col2)** - Returns a new Column for the sample covariance of col1 and col2\n",
    "\n",
    "Most of the aggregation function names are self-explaining, so nothing to comment on. Just first() and last() are a bit special. In contrast to most of the other aggregation functions, `first()` and `last()` both refer to the value **position** in the dataset and not to the value **amount**, like `min()` and `max()` do. So `first()` and `last()` are the only aggregation functions, being affected by data sorting.\n",
    "\n",
    "Back to data profiling. The ratio between count and countDistinct is an important indicator to identify key candidate columns. For primary keys, the ratio must be 1, i.e. countDistinct must equal the total count of values so it's cardinality must be also 1 to ensure uniqueness. \n",
    "\n",
    "Even though not beeing unique, columns with low cardinality are still candidates for performant data acess patterns. The cardinality is a measure of the average number of rows I will get when filtering in such a column value.\n",
    "\n",
    "The reverse value of cardinality, the entropy, is an indicator how compressible a column is. `DataFrames` having many columns with low entropy benefit much from a columnar storage format. On the other extreme, primary key columns are not compressible at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " count         | 541909               \n",
      " countDistinct | 5827                 \n",
      " cardinality   | 92.99965677020765    \n",
      " entropy       | 0.010752727856522036 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"Amount\").alias(\"count\"), \n",
    "        countDistinct(\"Amount\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprisingly the amount column is not a key candidate but it is very interesting, that the cardinality is quite high. Maybe there are only a few standard unit prices and/or lot sizes I can put orders on. Let's compare it with the *InvoiceNO* column. The column name sounds like a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " count         | 541909               \n",
      " countDistinct | 25900                \n",
      " cardinality   | 20.923127413127414   \n",
      " entropy       | 0.047794002314041656 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        count(\"InvoiceNo\").alias(\"count\"), \n",
    "        countDistinct(\"InvoiceNo\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this column has a much lower cardinality but stii it is not a unique key. The reason is, that the retail dataset is denormalized and the granularity is not based on invoices but on stock items. Since each invoice can list multiple stock items, I need to combine InviceNo and StockCode to get a unique key. Let's check, if this solves my problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "|          Key|InvoiceNO|        InvoiceDate|StockCode|Quantity|UnitPrice|Amount|CustomerID|       Country|\n",
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "| 580538-23084|   580538|2011-12-05 08:38:00|    23084|      48|     1.79| 85.92|     14075|United Kingdom|\n",
      "| 580538-23077|   580538|2011-12-05 08:38:00|    23077|      20|     1.25|  25.0|     14075|United Kingdom|\n",
      "| 580538-22906|   580538|2011-12-05 08:38:00|    22906|      24|     1.65|  39.6|     14075|United Kingdom|\n",
      "| 580538-21914|   580538|2011-12-05 08:38:00|    21914|      24|     1.25|  30.0|     14075|United Kingdom|\n",
      "| 580538-22467|   580538|2011-12-05 08:38:00|    22467|       6|     2.55|  15.3|     14075|United Kingdom|\n",
      "| 580538-21544|   580538|2011-12-05 08:38:00|    21544|      48|     0.85|  40.8|     14075|United Kingdom|\n",
      "| 580538-23126|   580538|2011-12-05 08:38:00|    23126|       8|     4.95|  39.6|     14075|United Kingdom|\n",
      "| 580538-21833|   580538|2011-12-05 08:38:00|    21833|      24|     1.69| 40.56|     14075|United Kingdom|\n",
      "| 580539-21479|   580539|2011-12-05 08:39:00|    21479|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "|580539-84030E|   580539|2011-12-05 08:39:00|   84030E|       4|     4.25|  17.0|     18180|United Kingdom|\n",
      "+-------------+---------+-------------------+---------+--------+---------+------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyedDf = preparedDf.select(\n",
    "    concat_ws('-', \"InvoiceNO\",\"StockCode\").alias(\"Key\"),\n",
    "    \"InvoiceNO\", \n",
    "    \"InvoiceDate\",\n",
    "    \"StockCode\", \n",
    "    \"Quantity\", \n",
    "    \"UnitPrice\",\n",
    "    \"Amount\",\n",
    "    \"CustomerID\", \n",
    "    \"Country\")\n",
    "\n",
    "keyedDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " count         | 541909             \n",
      " countDistinct | 531225             \n",
      " cardinality   | 1.0201120052708363 \n",
      " entropy       | 0.9802845127133891 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyedDf\\\n",
    "    .select(\n",
    "        count(\"Key\").alias(\"count\"), \n",
    "        countDistinct(\"Key\").alias(\"countDistinct\")\n",
    "    )\\\n",
    "    .withColumn(\"cardinality\", col(\"count\") / col(\"countDistinct\"))\\\n",
    "    .withColumn(\"entropy\", col(\"countdistinct\") / col(\"count\"))\\\n",
    "    .show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mh, I'm getting close but there are still some duplicates. Maybe there are some Null values in these columns. I would need ot investigate it furthr down, but I leave this for now because another phenomenon confuses me, there are two versions of counting in Spark:\n",
    "* `DataFrame.count()`\n",
    "* `pyspark.sql.functions.count()`\n",
    "\n",
    "The `DataFrame.count()` method is always applied on the entire `DataFrame` and counts the total number of physical rows in it. Additionally this method is an action and not a transformation, because the count number is directy determined and returned. On the other hand `pyspark.sql.functions.count()` is an aggregation function counting non-Null values which is applied on grouped data defined by a grouping key `DataFrame.groupBy()`or a window function. Aggregation functions define lazly evaluated transformations. \n",
    "## Aggregating on Grouped Data\n",
    "Aggregating on the entire DataFrame will show me just in row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+--------+------------------+------------------+\n",
      "|              sum|      min|     max|               avg|              mean|\n",
      "+-----------------+---------+--------+------------------+------------------+\n",
      "|9747747.929999745|-168469.6|168469.6|17.987794869617858|17.987794869617858|\n",
      "+-----------------+---------+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .select(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such highly aggregated data does not provide me much business insight, so I want to see the results for each country. So the first thing I have to do is to define a grouping key to arrange the data to get one group for each country. Than I can aggregate on each group seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           Country|count|\n",
      "+------------------+-----+\n",
      "|            Sweden|  462|\n",
      "|         Singapore|  229|\n",
      "|           Germany| 9495|\n",
      "|               RSA|   58|\n",
      "|            France| 8557|\n",
      "|            Greece|  146|\n",
      "|European Community|   61|\n",
      "|           Belgium| 2069|\n",
      "|           Finland|  695|\n",
      "|             Malta|  127|\n",
      "+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.groupBy(\"Country\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I already know, rearranging data means, Spark is shuffling partitions around. The explain plan confirms this (*Exchange hashpartitioning*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparedDf.groupBy(\"Country\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|           Country|               sum|     min|    max|               avg|              mean|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|            Sweden|36595.909999999996| -1188.0| 1188.0|  79.2119264069264|  79.2119264069264|\n",
      "|         Singapore|           9120.39|-3949.32|3949.32| 39.82703056768559| 39.82703056768559|\n",
      "|           Germany|         221698.21|  -599.5|  876.0|23.348942601369142|23.348942601369142|\n",
      "|               RSA|1002.3099999999998|     0.0|  38.25|17.281206896551723|17.281206896551723|\n",
      "|            France|197403.90000000008|-8322.12|4161.06|   23.069288301975|   23.069288301975|\n",
      "|            Greece|           4710.52|   -50.0|  175.2| 32.26383561643836| 32.26383561643836|\n",
      "|European Community|1291.7500000000002|    -8.5|   60.0|21.176229508196727|21.176229508196727|\n",
      "|           Belgium|          40910.96|  -19.95|  165.0|19.773301111648138|19.773301111648138|\n",
      "|           Finland|22326.739999999994|   -80.0|  551.2| 32.12480575539568| 32.12480575539568|\n",
      "|             Malta|           2505.47|  -130.0|  455.0| 19.72811023622047| 19.72811023622047|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf\\\n",
    "    .groupBy(\"Country\")\\\n",
    "    .agg(\n",
    "        sum(\"Amount\").alias(\"sum\"), \n",
    "        min(\"Amount\").alias(\"min\"), \n",
    "        max(\"Amount\").alias(\"max\"), \n",
    "        avg(\"Amount\").alias(\"avg\"), \n",
    "        mean(\"Amount\").alias(\"mean\")\n",
    "    )\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to remind myself: I could do this all using my good-old SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|           Country|               sum|     min|    max|               avg|              mean|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "|            Sweden|36595.909999999996| -1188.0| 1188.0|  79.2119264069264|  79.2119264069264|\n",
      "|         Singapore|           9120.39|-3949.32|3949.32| 39.82703056768559| 39.82703056768559|\n",
      "|           Germany|         221698.21|  -599.5|  876.0|23.348942601369142|23.348942601369142|\n",
      "|               RSA|1002.3099999999998|     0.0|  38.25|17.281206896551723|17.281206896551723|\n",
      "|            France|197403.90000000008|-8322.12|4161.06|   23.069288301975|   23.069288301975|\n",
      "|            Greece|           4710.52|   -50.0|  175.2| 32.26383561643836| 32.26383561643836|\n",
      "|European Community|1291.7500000000002|    -8.5|   60.0|21.176229508196727|21.176229508196727|\n",
      "|           Belgium|          40910.96|  -19.95|  165.0|19.773301111648138|19.773301111648138|\n",
      "|           Finland|22326.739999999994|   -80.0|  551.2| 32.12480575539568| 32.12480575539568|\n",
      "|             Malta|           2505.47|  -130.0|  455.0| 19.72811023622047| 19.72811023622047|\n",
      "+------------------+------------------+--------+-------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedDf.createOrReplaceTempView(\"retailTable\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Country, sum(Amount) as sum, min(Amount) as min, max(Amount) as max, \n",
    "        avg(Amount) as avg, mean(Amount) as mean \n",
    "    FROM retailTable\n",
    "    GROUP BY Country\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating on Floating Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
