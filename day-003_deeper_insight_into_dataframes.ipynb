{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - A Deeper Insight into DataFrames\n",
    "\n",
    "Link to <a href=\"https://spark.apache.org/docs/2.4.5/api/python/index.html\">pyspark Doc</a>\n",
    "\n",
    "My mission for today is to get a better understandig of `DataFrame` objects, their structure and how to operate with them. Since handling data of different shapes was always a challange in my life as ETL developer so far, I start with the schema topic, which already came across yesterday.\n",
    "\n",
    "## Schemas\n",
    "\n",
    "So far, I've learned two things about schemas in Spark. First, they define the names and types of `DataFrame` `Columns`. However Spark is using internal types of its Catalyst language regardless of the API language I'm using. Second, I can ask Spark to derive the schema from a source file or I can explicitly define the schema of the data I want to process.\n",
    "\n",
    "Spark derives the schema by just reading a small sample of data in the file, which might not be representitive enough for the entire dataset. So maybe for production purposes it might be a better idea to express my expectation explicitly of how the data I want to process and analyse is actually shaped.\n",
    "\n",
    "How does schemas look like in Spark? The`printSchema()` function will help me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()\n",
    "\n",
    "csvData = spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.csv\")\\\n",
    "   .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonData = spark.read\\\n",
    "   .format(\"json\")\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.json\")\\\n",
    "   .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the first thing I notice here is, even though both files have different format, CSV vs. JSON, they have nearly identical schema, i.e. having the same column names and types. The only difference is, that  Spark interprets values in the CSV file as strings and not as `long` numbers.\n",
    "\n",
    "To find out how to define a schema in Spark, I have look at, how Spark does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,StringType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.csv\")\\\n",
    "   .schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\\\n",
    "   .format(\"json\")\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.json\")\\\n",
    "   .schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have the blueprints to create similar schemas by myself, e.g. to define the \"count\" column as optional. One tricky aspect here is, that to define a schema I need to call the constructor methods of the Spark internal types, e.g. `StringType()`, just using the typename like `StringType` won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myOwnCsv = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",StringType(),False)\n",
    "])\n",
    "\n",
    "myOwnJson = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",LongType(),False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since my schema definitions are less restrictive, the file load should also work when I enforce them explicitly by calling the `schema()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .schema(myOwnCsv)\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.csv\")\\\n",
    "   .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "   .format(\"json\")\\\n",
    "   .schema(myOwnJson)\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.json\")\\\n",
    "   .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they do.\n",
    "## Columns and Expressions\n",
    "The `DataFrame` API provides two ways how to create `Column` objects using either the `col()` or the `expr()` function, which confuses me at first sight. If I apply both of them on the \"count\" column of my test data, I get exactly the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions  import col, expr\n",
    "df = spark.read\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .format(\"csv\")\\\n",
    "   .schema(myOwnCsv)\\\n",
    "   .load(\"./data/day-003/flight-data/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|(count - 10)|\n",
      "+-------------------+-----+------------+\n",
      "|            Romania|   15|         5.0|\n",
      "|            Croatia|    1|        -9.0|\n",
      "|            Ireland|  344|       334.0|\n",
      "|      United States|   15|         5.0|\n",
      "|              India|   62|        52.0|\n",
      "+-------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"ORIGIN_COUNTRY_NAME\"), col(\"count\"), col(\"count\") -10).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|(count - 10)|\n",
      "+-------------------+-----+------------+\n",
      "|            Romania|   15|         5.0|\n",
      "|            Croatia|    1|        -9.0|\n",
      "|            Ireland|  344|       334.0|\n",
      "|      United States|   15|         5.0|\n",
      "|              India|   62|        52.0|\n",
      "+-------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"ORIGIN_COUNTRY_NAME\"), expr(\"count\"), expr(\"count - 10\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok Spark, explain to me what's going on behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [ORIGIN_COUNTRY_NAME#95, count#96, (cast(count#96 as double) - 10.0) AS (count - 10)#132]\n",
      "+- *(1) FileScan csv [ORIGIN_COUNTRY_NAME#95,count#96] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/Git/pyspark/pyspark-tutorial/data/day-003/flight-data/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string,count:string>\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"ORIGIN_COUNTRY_NAME\"), col(\"count\"), col(\"count\") - 10).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [ORIGIN_COUNTRY_NAME#95, count#96, (cast(count#96 as double) - 10.0) AS (count - 10)#136]\n",
      "+- *(1) FileScan csv [ORIGIN_COUNTRY_NAME#95,count#96] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/Git/pyspark/pyspark-tutorial/data/day-003/flight-data/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string,count:string>\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"ORIGIN_COUNTRY_NAME\"), expr(\"count\"), expr(\"count - 10\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [ORIGIN_COUNTRY_NAME#95, count#96, (cast(count#96 as double) - 10.0) AS (count - 10)#140]\n",
      "+- *(1) FileScan csv [ORIGIN_COUNTRY_NAME#95,count#96] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/Git/pyspark/pyspark-tutorial/data/day-003/flight-data/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string,count:string>\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"ORIGIN_COUNTRY_NAME\", \"count\", \"count - 10\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [ORIGIN_COUNTRY_NAME#95, count#96, (cast(count#96 as double) - 10.0) AS (CAST(count AS DOUBLE) - CAST(10 AS DOUBLE))#144]\n",
      "+- *(1) FileScan csv [ORIGIN_COUNTRY_NAME#95,count#96] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oli/Git/pyspark/pyspark-tutorial/data/day-003/flight-data/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ORIGIN_COUNTRY_NAME:string,count:string>\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table\")\n",
    "spark.sql(\"\"\"SELECT ORIGIN_COUNTRY_NAME, count, count -10 FROM table\"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I've learned on day 2, any functional transformation on `DataFrame` objects is equivalent to SQL queries on tables, so `col(\"columnName\")` is equivalent to the projection in relational algebra in terms of, take the third data element of each record (row). So finally there are three equivalent ways how to express transformations in Spark:\n",
    "* as a composition of funtion: `select(col(\"count\") - 10)`\n",
    "* as an expression String: `select(expr(\"count - 10\")* or *selectExpr(\"count - 10\"))`\n",
    "* as SQL query: `sql(\"\"\"SELECT count - 10 FROM\"\"\")`\n",
    "\n",
    "`DataFrame` objects have attributes describing their metadata. The attribute `column` provides a list of all column names of the given `DataFrame`. Python lists are iterable objects so I can use this attribute to iterate over all column names of a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records and Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records are logical sets of related data values. In Spark they are technically represented by `Row` objects.\n",
    "\n",
    "Spark `DataFrame` objects are collections of `Row` objects all having the same structure according to the schema of the `DataFrame`. I can create a `Row` object by myself, even independently from any schema or the existens of a `DataFrame`, which is not possible for `Column` objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "myOwnRow = Row(42, \"is the answer to all questions\", True)\n",
    "type(myOwnRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now `DataFrame` so no schema yet, but I have a row now. Like with Python lists I can access the `Row` data elements by their positional index starting with 0 for the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is the answer to all questions'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myOwnRow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the schema comes back into place as soon as I want to collect multiple records into the same `DataFrame` because than, all records must comply with the same schema.\n",
    "\n",
    "Next to creating DataFrames on the fly from source files, like I did so far, I could also create my `DataFrame` by my own. This get's relevant to me, when there is no data source because I created the data by myself, e.g. simulated scenario data. All I need to is creating a schema, like I did on day 2 an some `Row` objects like I created just before and compile both into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, BooleanType\n",
    "\n",
    "myOwnSchema = StructType([\n",
    "    StructField(\"ID\", LongType(), True),\n",
    "    StructField(\"Message\", StringType(), True),\n",
    "    StructField(\"is true or false\", BooleanType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySecondRow = Row(73, \"is a prime number\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+\n",
      "| ID|             Message|is true or false|\n",
      "+---+--------------------+----------------+\n",
      "| 42|is the answer to ...|            true|\n",
      "| 73|   is a prime number|            true|\n",
      "+---+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF = spark.createDataFrame([myOwnRow, mySecondRow], myOwnSchema)\n",
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFrame` custructor `createDataFrame()` accepts a list of `Row` objects, so I don't have to insert rows manually one-by-one but can handover a list objects which I've maybe generated at another place in my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------------+\n",
      "| ID|             Message|is true or false|\n",
      "+---+--------------------+----------------+\n",
      "| 42|is the answer to ...|            true|\n",
      "| 73|   is a prime number|            true|\n",
      "+---+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rowList = [myOwnRow, mySecondRow]\n",
    "myDF = spark.createDataFrame(rowList, myOwnSchema)\n",
    "myDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations\n",
    "Eventhough rows are individual objects, Spark does not manipulate them individually. To keep mass data processing fast, `Row` objects are manipulated by column expressions applied on `DataFrame`, which in fact are collections of `Row` objects. Again this is equivalent to the relational algebra of SQL SELECT queries. Here are some common example, I'm familiar with in SQL, but how do they look like in the functional format?\n",
    "### Adding a Column with Literal Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| ID|             Message|literal|\n",
      "+---+--------------------+-------+\n",
      "| 42|is the answer to ...|     23|\n",
      "| 73|   is a prime number|     23|\n",
      "+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "myDF\\\n",
    "   .select(col(\"ID\"), col(\"Message\"))\\\n",
    "   .withColumn(\"literal\", lit(23))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, I could use `alias()` instead and create the new column inside of `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| ID|             Message|literal|\n",
      "+---+--------------------+-------+\n",
      "| 42|is the answer to ...|     23|\n",
      "| 73|   is a prime number|     23|\n",
      "+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\"), col(\"Message\"), lit(23).alias(\"literal\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I appreaciate the readability when using `withColumn()`, especually in more complex queries. On the other hand it is less flexible because new columns get always appended to the right whereas with the second approach, using `alias()`, I can put the new `Column` at any position in the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ff87c668faab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDF\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"literal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myDF' is not defined"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\"), lit(23).alias(\"literal\"), col(\"Message\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Calculated Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+\n",
      "| ID|             Message|Calculated ID|\n",
      "+---+--------------------+-------------+\n",
      "| 42|is the answer to ...|         4200|\n",
      "| 73|   is a prime number|         7300|\n",
      "+---+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\"), col(\"Message\"))\\\n",
    "   .withColumn(\"Calculated ID\", col(\"ID\") * 100)\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------+\n",
      "| ID|             Message|ID calculated|\n",
      "+---+--------------------+-------------+\n",
      "| 42|is the answer to ...|         4200|\n",
      "| 73|   is a prime number|         7300|\n",
      "+---+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\"), col(\"Message\"), (col(\"ID\") * 100).alias(\"ID calculated\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both versions the new Column \"ID Calculated\" is the result of three consecutive expressions\n",
    "* `col(\"ID\")`, i.e. take first data element of each row in the input DataFrame\n",
    "* `* 100`, i.e. multiply each data value by 100\n",
    "* `alias(\"ID calculated\")` to rename the new column\n",
    "\n",
    "The first two are data manipulating whereas the third one manipulates the metadata of the `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Identifier|             Message|\n",
      "+----------+--------------------+\n",
      "|        42|is the answer to ...|\n",
      "|        73|   is a prime number|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\"), col(\"Message\"))\\\n",
    "   .withColumnRenamed(\"ID\", \"Identifier\")\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|Identifier|             Message|\n",
      "+----------+--------------------+\n",
      "|        42|is the answer to ...|\n",
      "|        73|   is a prime number|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\").alias(\"Identifier\"), col(\"Message\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my opinion, the Spark syntax regarding column names is quite misleading. Even though I have to put the column name argument in quotation marks and eventhough the output shows me the column name with exactly the same upper/lower cases, as I defined it, Spark is **NOT** case sensitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             Message|             MESSAGE|\n",
      "+--------------------+--------------------+\n",
      "|is the answer to ...|is the answer to ...|\n",
      "|   is a prime number|   is a prime number|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"Message\"), col(\"MESSAGE\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I can switch to case sensitivity by changing the `SparkSession` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "|4200|\n",
      "|7300|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF\\\n",
    "   .select(col(\"ID\"), (col(\"ID\") * 100).alias(\"id\"))\\\n",
    "   .select(col(\"id\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names with reserved character or keywords can also be very tricky because, as I've seen befor, Spark does not interprete everything in quation marks literally as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "| 73|   is a prime number|       7300|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames = myDF\\\n",
    "   .select(col(\"ID\"), col(\"Message\"))\\\n",
    "   .withColumn(\"ID * by 100\", col(\"ID\") * 100)\n",
    "\n",
    "dfWithSpecialNames.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, how can I reference the third column without getting problems. First of all the direct string to column reference by using `col()` works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|ID * by 100|\n",
      "+-----------+\n",
      "|       4200|\n",
      "|       7300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "   .select(col(\"ID * by 100\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|(ID * by 100 * 100)|\n",
      "+-------------------+\n",
      "|             420000|\n",
      "|             730000|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "   .select(col(\"ID * by 100\") * 100)\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this reference also work in expression strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input '100' expecting <EOF>(line 1, pos 8)\\n\\n== SQL ==\\nID * by 100  * 100\\n--------^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/python/python3-7/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o868.selectExpr.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input '100' expecting <EOF>(line 1, pos 8)\n\n== SQL ==\nID * by 100  * 100\n--------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:44)\n\tat org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1376)\n\tat org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1375)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1375)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-a8fc81aa543d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdfWithSpecialNames\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ID * by 100  * 100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/python3-7/pyspark/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input '100' expecting <EOF>(line 1, pos 8)\\n\\n== SQL ==\\nID * by 100  * 100\\n--------^^^\\n\""
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "   .selectExpr(\"ID * by 100  * 100\")\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, something got broken. Now it is undefined, which parts of the expression string are operators and which are literal stings valus.  This problem will get very relevant when it comes to filter or join conditions, so I need a solution. Spark provides the back stick character to escape the column name strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|(ID * by 100 * 100)|\n",
      "+-------------------+\n",
      "|             420000|\n",
      "|             730000|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "   .selectExpr(\"`ID * by 100`  * 100\")\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, my lessons learned her is: avoid blanks, reserved characters or keywords in column names whenever possible.\n",
    "### Removing Columns\n",
    "In the SQL world, there is a shortcut to grep all columns using the wildcard, which is very comfortable querying a table with, e.g. 100 columns. What I'm realy missing, is the option to exclude just one or two columns and get the remaining 98 columns without having to list them all in the SELECT clause. So I'm very happy to see that Spark provides me this feature by the `drop()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| ID|             Message|\n",
      "+---+--------------------+\n",
      "| 42|is the answer to ...|\n",
      "| 73|   is a prime number|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames.drop(\"ID * by 100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Rows\n",
    "Spark provides in total four ways of filtering rows. First, the two methonds `filter()` and `where()` are synonyms in pyspark. Second, the filter condition can either be a boolean column created by column manipualtion or an expression string, which evaluates to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames.filter(col(\"ID\") == 42).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames.filter(\"ID = 42\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames.where(col(\"ID\") == 42).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames.where(\"ID == 42\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think, the last version looks most familiar to me as an senior SQL user. A nice feature is, that I can chain up multiple AND filters, which is more readable than complex in-line expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "    .where(\"ID == 42\")\\\n",
    "    .where(\"`ID * by 100`== 4200\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "    .where(\"ID == 42 and `ID * by 100`== 4200\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, ass soon as OR comes into place, there is no choice anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+\n",
      "| ID|             Message|ID * by 100|\n",
      "+---+--------------------+-----------+\n",
      "| 42|is the answer to ...|       4200|\n",
      "| 73|   is a prime number|       7300|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithSpecialNames\\\n",
    "    .where(\"ID == 42 or ID == 73\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Samples and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "   .builder\\\n",
    "   .getOrCreate()\n",
    "\n",
    "df = spark.read\\\n",
    "   .format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(\"./data/day-003/retail-data/by-day/*.csv\")\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.seed()\n",
    "withReplacement = False\n",
    "fraction = 0.1 # i.e. 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54775"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580539|    22074|6 RIBBONS SHIMMER...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
      "|   580539|    22075|6 RIBBONS ELEGANT...|      24|2011-12-05 08:39:00|     0.39|   18180.0|United Kingdom|\n",
      "|   580539|    23543| WALL ART KEEP CALM |       6|2011-12-05 08:39:00|     4.15|   18180.0|United Kingdom|\n",
      "|   580540|    23497|CLASSIC CHROME BI...|      12|2011-12-05 08:49:00|     1.45|   13417.0|United Kingdom|\n",
      "|   580540|    22423|REGENCY CAKESTAND...|       1|2011-12-05 08:49:00|    12.75|   13417.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(withReplacement, fraction, seed).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's `ramdomSplit()` feature can be very usefull if I want to train a neural network model where spliting up a dataset by random into two samples, the training data and the test data, is an important datap reparation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.seed()\n",
    "mlData = df.randomSplit([0.8, 0.2], seed)\n",
    "traingData = mlData[0]\n",
    "testData = mlData[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other DataFrame Transformations\n",
    "The following functions are nearly the same as I already know from SQL. So I skip them here.\n",
    "\n",
    "* `df1.count()`\n",
    "* `df1.distinct().count()`\n",
    "* `df1.union(df2)`\n",
    "* `df1.orderBy(... desc, ... asc)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning Data\n",
    "As I've learnd on day 2, wide transformations, forces Spark to shuffel data across the cluster which can end up in performance issues since Spark cannot do it in-memory. Therefore I can improve the performance, if I partition the data according to my expected query patterns. \n",
    "\n",
    "Ok, let's see how Spark has partitioned my data, I've read from the daily retail csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are 12 partitions. I want to optimaize the partitioning for queries on stock codes. How many Stock codes do I have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4070"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"StockCode\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option would be, to raise the number of partitions. But this doesn't help if I have not enough CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.repartition(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, just raising the number of partitions would not ensure, that all rows having the same StockCode value which reside in the same partition. The later is the crucial point to gain partition locality of the query processing. So I have to define the StockCode column as partition criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"StockCode\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, If I had some further CPU cores at hand, can raise the number of partitions additionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(20, col(\"StockCode\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Rows to Local Driver\n",
    "Especially when I do some ad-hoc analysis of the data on my local machine, it is quite usefull to limit the data I' fetching from the cluster.\n",
    "\n",
    "`collect()` returns all the records as a list of `Row` objects. In Python, lists are iterable sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`limit(num)` limits the result count to the specified number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='580538', StockCode='23084', Description='RABBIT NIGHT LIGHT', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.79, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='23077', Description='DOUGHNUT LIP GLOSS ', Quantity=20, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='22906', Description='12 MESSAGE CARDS WITH ENVELOPES', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.65, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='21914', Description='BLUE HARMONICA IN BOX ', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='22467', Description='GUMBALL COAT RACK', Quantity=6, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=2.55, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='21544', Description='SKULLS  WATER TRANSFER TATTOOS ', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=0.85, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='23126', Description='FELTCRAFT GIRL AMELIE KIT', Quantity=8, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=4.95, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='21833', Description='CAMOUFLAGE LED TORCH', Quantity=24, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.69, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580539', StockCode='21479', Description='WHITE SKULL HOT WATER BOTTLE ', Quantity=4, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 39), UnitPrice=4.25, CustomerID=18180.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580539', StockCode='84030E', Description='ENGLISH ROSE HOT WATER BOTTLE', Quantity=4, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 39), UnitPrice=4.25, CustomerID=18180.0, Country='United Kingdom')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`take(num)` returns just the first *num* rows as a list of Row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='580538', StockCode='23084', Description='RABBIT NIGHT LIGHT', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.79, CustomerID=14075.0, Country='United Kingdom'),\n",
       " Row(InvoiceNo='580538', StockCode='23077', Description='DOUGHNUT LIP GLOSS ', Quantity=20, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.25, CustomerID=14075.0, Country='United Kingdom')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I just want the first row on top, I can do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(InvoiceNo='580538', StockCode='23084', Description='RABBIT NIGHT LIGHT', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.79, CustomerID=14075.0, Country='United Kingdom')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`toLocalIterator()` returns an iterator that contains all of the rows in this `DataFrame`. The iterator will consume as much memory as the largest partition in this `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think, that's enough for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
